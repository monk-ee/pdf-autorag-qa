name: PDF Q&A AutoRAG Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'pdfs/**'
      - 'qa_extraction_lib/**'
      - 'cli_pdf_qa.py'
      - 'domain_eval_gpu.py'
  workflow_dispatch:
    inputs:
      input_file:
        description: 'Input PDF file path'
        required: true
        default: 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf'
      model_name:
        description: 'Hugging Face model name'
        required: false
        default: 'meta-llama/Meta-Llama-3-8B-Instruct'
      chunk_size:
        description: 'Chunk size in words'
        required: false
        default: '800'
      batch_size:
        description: 'Batch size for processing'
        required: false
        default: '8'
      use_quantization:
        description: 'Use 4-bit quantization to save GPU memory'
        required: false
        default: 'false'
      max_questions:
        description: 'Maximum number of evaluation questions'
        required: false
        default: '15'
      enable_bert_score:
        description: 'Enable BERT-score evaluation'
        required: false
        default: 'true'
      top_k_selection:
        description: 'Top K Q&A pairs to select for AutoRAG evaluation'
        required: false
        default: '50'

jobs:
  # Serial PDF Q&A Generation + AutoRAG Pipeline (All permutations in one job)
  pdf-qa-autorag-serial:
    runs-on:
      - machine
      - gpu=l40s
      - cpu=8
      - ram=64
      - architecture=x64
      - tenancy=spot
    timeout-minutes: 360
    env:
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      HF_HUB_ENABLE_HF_TRANSFER: 1
      HF_HUB_DOWNLOAD_TIMEOUT: 120
      CUDA_LAUNCH_BLOCKING: 1
      TORCH_USE_CUDA_DSA: 1
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        enable-cache: true
        cache-dependency-glob: "pyproject.toml"

    - name: Install dependencies with uv
      run: |    
        # Install all dependencies from pyproject.toml
        uv sync
        
        # Install GPU-specific dependencies with better detection
        echo "üîç Checking GPU availability..."
        nvidia-smi || echo "No NVIDIA GPU detected"
        
        # SIMPLE SOLUTION: Use Python 3.10 with FAISS-GPU (no conda bullshit)
        echo "üöÄ SIMPLE FAISS-GPU: Using Python 3.10 (supported by faiss-gpu)"
        
        if nvidia-smi > /dev/null 2>&1; then
          echo "üí™ GPU detected - installing FAISS-GPU with CUDA 12.x support..."
          
          # Check CUDA version and install compatible FAISS-GPU
          cuda_version=$(nvidia-smi | grep "CUDA Version" | awk '{print $9}' | cut -d'.' -f1,2)
          echo "üîç Detected CUDA version: $cuda_version"
          
          # Try CUDA 12.x compatible versions first
          echo "üöÄ Installing FAISS-GPU for CUDA 12.x..."
          pip install faiss-gpu --index-url https://download.pytorch.org/whl/cu121 || \
          pip install faiss-gpu --index-url https://download.pytorch.org/whl/cu118 || \
          uv pip install faiss-gpu || {
            echo "‚ùå All FAISS-GPU installations failed, using faiss-cpu"
            uv pip install faiss-cpu>=1.7.4
          }
        else
          echo "üíª No GPU detected, installing faiss-cpu..."
          uv pip install faiss-cpu>=1.7.4
        fi
        
        # Install other dependencies
        uv pip install bitsandbytes>=0.41.0 sentence-transformers>=2.2.0
        
        # Verify critical imports and GPU detection
        uv run python -c "import sentence_transformers; print('‚úÖ sentence-transformers:', sentence_transformers.__version__)"
        uv run python -c "import torch; print('‚úÖ torch:', torch.__version__, '| CUDA available:', torch.cuda.is_available(), '| Device count:', torch.cuda.device_count())"
        uv run python -c "import transformers; print('‚úÖ transformers:', transformers.__version__)"
        uv run python -c "import bitsandbytes; print('‚úÖ bitsandbytes available for quantization')"
        uv run python -c "import faiss; print('‚úÖ faiss installed'); print('üîç FAISS GPU detection:', faiss.get_num_gpus() if hasattr(faiss, 'get_num_gpus') else 'N/A', 'GPUs available')"
    
    - name: Login to Hugging Face
      run: |
        uv run python -c "from huggingface_hub import login; login('${{ secrets.HF_TOKEN }}')"

    - name: Create output directories
      run: |
        mkdir -p outputs rag_input rag_store autorag_results
        echo "üìÅ Created pipeline directories"
        echo "üöÄ AutoRAG Pipeline Starting..."

    # Run all 9 permutations sequentially (model loads once and stays in memory)
    - name: "Q&A Generation - Basic √ó High Creativity"
      run: |
        uv run python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_basic_high_creativity_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.9 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels basic \
          --run-name "_basic_high_creativity" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Basic √ó Balanced"
      run: |
        uv run python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_basic_balanced_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.7 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels basic \
          --run-name "_basic_balanced" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Basic √ó Conservative"
      run: |
        uv run python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_basic_conservative_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 256 \
          --temperature 0.3 \
          --top-p 0.8 \
          --do-sample \
          --difficulty-levels basic \
          --run-name "_basic_conservative" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Intermediate √ó High Creativity"
      run: |
        uv run python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_intermediate_high_creativity_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.9 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels intermediate \
          --run-name "_intermediate_high_creativity" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Intermediate √ó Balanced"
      run: |
        uv run python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_intermediate_balanced_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.7 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels intermediate \
          --run-name "_intermediate_balanced" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Intermediate √ó Conservative"
      run: |
        uv run python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_intermediate_conservative_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 256 \
          --temperature 0.3 \
          --top-p 0.8 \
          --do-sample \
          --difficulty-levels intermediate \
          --run-name "_intermediate_conservative" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Advanced √ó High Creativity"
      run: |
        uv run python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_advanced_high_creativity_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.9 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels advanced \
          --run-name "_advanced_high_creativity" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Advanced √ó Balanced"
      run: |
        uv run python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_advanced_balanced_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 512 \
          --temperature 0.7 \
          --top-p 0.9 \
          --do-sample \
          --difficulty-levels advanced \
          --run-name "_advanced_balanced" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: "Q&A Generation - Advanced √ó Conservative"
      run: |
        uv run python cli_pdf_qa.py \
          ${{ github.event.inputs.input_file || 'pdfs/UAFX_Ruby_63_Top_Boost_Amplifier_Manual.pdf' }} \
          --output outputs/pdf_qa_advanced_conservative_$(date +%Y%m%d_%H%M%S).jsonl \
          --model ${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }} \
          --chunk-size ${{ github.event.inputs.chunk_size || '800' }} \
          --batch-size ${{ github.event.inputs.batch_size || '8' }} \
          --max-new-tokens 256 \
          --temperature 0.3 \
          --top-p 0.8 \
          --do-sample \
          --difficulty-levels advanced \
          --run-name "_advanced_conservative" \
          --debug-prompts \
          --debug-chunks \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --verbose

    - name: Q&A Generation Summary
      run: |
        echo "üéØ ============================================"
        echo "   Q&A GENERATION COMPLETED - ALL PERMUTATIONS"
        echo "=============================================="
        
        echo "üìä Generated Q&A Files:"
        find outputs/ -name "*.jsonl" -type f -exec basename {} \; | sort
        
        echo ""
        echo "üìà Q&A Pairs Summary:"
        find outputs/ -name "*.jsonl" -type f -exec wc -l {} + | tail -1
        
        echo ""
        echo "üöÄ Starting AutoRAG pipeline with all permutation results..."

    # Continue with AutoRAG pipeline using all generated results
    - name: Create AutoRAG Input Directory
      run: |
        echo "üìÅ Preparing AutoRAG input from all permutations"
        
        # List all generated Q&A files
        echo "üì• Q&A Generation Results:"
        find outputs/ -name "*.jsonl" -type f | head -20
        
        # Count total Q&A pairs across all matrix combinations
        echo "üìä Q&A Pairs Summary:"
        find outputs/ -name "*.jsonl" -type f -exec wc -l {} + | tail -1
    
    - name: Select Top K Q&A Pairs
      run: |
        uv run python qa_pair_selector.py \
          --qa-artifacts-dir outputs \
          --output-dir rag_input \
          --top-k ${{ github.event.inputs.top_k_selection || '50' }} \
          --verbose
    
    - name: Build Q&A Vector Stores (Standard + Adaptive RAG)
      run: |
        uv run python qa_faiss_builder.py \
          --qa-pairs-file rag_input/selected_qa_pairs.json \
          --output-dir rag_store \
          --embedding-model all-MiniLM-L6-v2 \
          --verbose
    
    - name: Run Standard RAG Evaluation
      run: |
        echo "üîπ STANDARD RAG EVALUATION"
        echo "=========================="
        
        # Use GPU standard index if available, fallback to CPU
        if [ -f "rag_store/qa_faiss_index_standard_gpu.bin" ]; then
          echo "üöÄ Using Standard GPU-optimized FAISS index"
          FAISS_INDEX="rag_store/qa_faiss_index_standard_gpu.bin"
        else
          echo "‚ö†Ô∏è Standard GPU index not found, using Standard CPU index"
          FAISS_INDEX="rag_store/qa_faiss_index_standard_cpu.bin"
        fi
        
        uv run python qa_autorag_evaluator.py \
          --qa-pairs-file rag_input/selected_qa_pairs.json \
          --qa-faiss-index "$FAISS_INDEX" \
          --qa-metadata rag_store/qa_metadata.json \
          --output-dir autorag_results/standard_rag \
          --model-name "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --embedding-model all-MiniLM-L6-v2 \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantization' || '' }} \
          --verbose
    
    - name: Run Enhanced Adaptive RAG Evaluation  
      run: |
        echo "üöÄ ENHANCED ADAPTIVE RAG EVALUATION"
        echo "===================================="
        echo "‚úÖ Cross-encoder re-ranking"
        echo "‚úÖ Hybrid dense+sparse retrieval"  
        echo "‚úÖ Dynamic context windows"
        echo "‚úÖ Enhanced query classification"
        echo "===================================="
        
        # Use Enhanced Adaptive RAG Evaluator (no FAISS index needed - uses internal pipeline)
        uv run python qa_enhanced_adaptive_evaluator.py \
          --qa-pairs-file rag_input/selected_qa_pairs.json \
          --output-dir autorag_results/adaptive_rag \
          --model-name "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --domain-config audio_equipment_domain_questions.json \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }} \
          --device auto
    
    - name: Compare Standard vs Adaptive RAG Performance
      run: |
        echo "üìä RAG APPROACH COMPARISON ANALYSIS"
        echo "===================================="
        
        uv run python rag_comparison_analyzer.py \
          --standard-results autorag_results/standard_rag \
          --adaptive-results autorag_results/adaptive_rag \
          --output-file autorag_results/rag_comparison_report.json \
          --verbose
    
    - name: Generate High-Quality Training Dataset
      run: |
        echo "üèóÔ∏è TRAINING DATASET GENERATION"
        echo "=============================="
        
        # Use the comparison report to determine which approach performed better
        COMPARISON_REPORT="autorag_results/rag_comparison_report.json"
        
        if [ -f "$COMPARISON_REPORT" ]; then
          WINNER=$(python -c "import json; print(json.load(open('$COMPARISON_REPORT'))['summary']['winner'])")
          echo "üèÜ RAG Comparison Winner: $WINNER"
          
          if [ "$WINNER" = "adaptive" ]; then
            EVAL_RESULTS="autorag_results/adaptive_rag/qa_rag_evaluation_report.json"
            echo "üìà Using Adaptive RAG results for training dataset"
          else
            EVAL_RESULTS="autorag_results/standard_rag/qa_rag_evaluation_report.json"
            echo "üìà Using Standard RAG results for training dataset"
          fi
        else
          echo "‚ö†Ô∏è No comparison report found, defaulting to Standard RAG"
          EVAL_RESULTS="autorag_results/standard_rag/qa_rag_evaluation_report.json"
        fi
        
        uv run python training_dataset_generator.py \
          --evaluation-results "$EVAL_RESULTS" \
          --output-dir autorag_results \
          --min-similarity 0.3 \
          --min-length-ratio 0.5 \
          --max-length-ratio 2.0 \
          --min-answer-length 20 \
          --verbose

    - name: Run Audio Equipment Domain Specificity Evaluation
      run: |
        uv run python domain_eval_gpu.py \
          --model "${{ github.event.inputs.model_name || 'meta-llama/Meta-Llama-3-8B-Instruct' }}" \
          --results-dir outputs \
          --config audio_equipment_domain_questions.json \
          --max-questions ${{ github.event.inputs.max_questions || '15' }} \
          ${{ github.event.inputs.enable_bert_score == 'false' && '--no-bert-score' || '' }} \
          ${{ github.event.inputs.use_quantization == 'true' && '--quantize' || '' }}

    # Single artifact upload at the end with all results
    - name: Upload Complete Pipeline Results
      uses: actions/upload-artifact@v4
      with:
        name: pdf-qa-autorag-dual-rag-complete
        path: |
          outputs/
          rag_input/
          rag_store/
          autorag_results/
          domain_eval_results.csv
          domain_eval_analysis.json
        retention-days: 30

    - name: Display Pipeline Summary
      run: |
        echo "üéØ ============================================"
        echo "   ENHANCED DUAL RAG Pipeline - FINAL RESULTS"
        echo "üöÄ Standard RAG vs Enhanced Adaptive RAG"
        echo "=============================================="
        
        echo "üì¶ FAISS INDICES CREATED:"
        echo "========================"
        find rag_store/ -name "*.bin" | sort | while read file; do
          echo "  ‚Ä¢ $(basename "$file")"
        done
        
        echo ""
        echo "üìä RAG APPROACH COMPARISON:"
        echo "=========================="
        if [ -f "autorag_results/rag_comparison_report.json" ]; then
          uv run python -c "
        import json
        with open('autorag_results/rag_comparison_report.json', 'r') as f:
            report = json.load(f)
        
        summary = report['summary']
        print(f'Winner: {summary[\"winner\"].upper()}')
        print(f'Key Finding: {summary[\"key_finding\"]}')
        print(f'Recommendation: {summary[\"recommendation_summary\"]}')
        print()
        
        # Show key metrics comparison
        differences = report['metrics_comparison']['differences']
        print('üìà Key Performance Differences:')
        for metric in ['quality_retention_rate', 'avg_bert_score', 'avg_context_relevance']:
            if metric in differences:
                diff = differences[metric]
                improvement = diff['improvement_percent']
                better = diff['better_approach']
                print(f'  {metric.replace(\"_\", \" \").title()}: {improvement:+.1f}% ({better} wins)')
          "
        else
          echo "‚ö†Ô∏è RAG comparison report not found"
        fi
        
        echo ""
        if [ -f "domain_eval_analysis.json" ]; then
          echo "üìà DOMAIN EVALUATION RESULTS:"
          echo "============================"
          uv run python -c "
        import json
        with open('domain_eval_analysis.json', 'r') as f:
            analysis = json.load(f)
        
        print(f'Domain Evaluation Questions: {analysis[\"total_questions\"]}')
        print(f'Base Model Domain Relevance: {analysis[\"avg_base_domain_relevance\"]:.3f}')
        print(f'RAG Model Domain Relevance: {analysis[\"avg_rag_domain_relevance\"]:.3f}')
        
        if 'avg_base_bert_f1' in analysis:
            print(f'Base Model BERT F1: {analysis[\"avg_base_bert_f1\"]:.3f}')
            print(f'RAG Model BERT F1: {analysis[\"avg_rag_bert_f1\"]:.3f}')
          "
        fi
        
        echo ""
        echo "üöÄ READY FOR LOCAL MODEL TRAINING:"
        echo "=================================="
        echo "‚úÖ High-quality Q&A dataset: autorag_results/high_quality_pairs.jsonl"
        echo "‚úÖ FAISS vector store: rag_store/faiss_index.bin"  
        echo "‚úÖ Evaluation reports: autorag_results/final_evaluation_report.json"
        echo "‚úÖ Domain analysis: domain_eval_analysis.json"
        echo ""
        echo "üí° Next Steps:"
        echo "  1. Download artifacts for local training"
        echo "  2. Use high_quality_pairs.jsonl for fine-tuning"
        echo "  3. Deploy FAISS index for RAG frontend"
        echo "  4. Monitor training with evaluation reports"