#!/usr/bin/env python3
"""
Adaptive RAG Pipeline: Dynamic retrieval and response optimization
Adjusts retrieval strategy and context based on query analysis and confidence scores
"""

import json
import logging
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, replace
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer, CrossEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine
import faiss
import re
from collections import Counter
from rank_bm25 import BM25Okapi

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class QueryAnalysis:
    """Analysis results for a query"""
    query_type: str  # factual, conceptual, procedural, technical, comparison
    domain_relevance: float  # 0-1 score
    complexity: str  # basic, intermediate, advanced
    uncertainty_required: bool
    key_concepts: List[str]
    confidence_threshold: float


@dataclass
class RetrievalStrategy:
    """Retrieval strategy configuration"""
    top_k: int
    alpha: float  # dense vs sparse weight
    rerank: bool
    context_window: int
    confidence_gating: bool
    fallback_strategy: str


@dataclass
class AdaptiveResponse:
    """Response with adaptive metadata"""
    answer: str
    confidence_scores: Dict[str, float]
    retrieval_used: RetrievalStrategy
    context_quality: float
    adaptive_adjustments: List[str]


class QueryAnalyzer:
    """Analyzes queries to determine optimal RAG strategy"""
    
    def __init__(self, domain_config_file: str = "audio_equipment_domain_questions.json", 
                 category_config_file: str = "adaptive_categories.json"):
        self.config_file = Path(domain_config_file)
        self.category_config_file = Path(category_config_file)
        self.config = self.load_config()
        self.category_config = self.load_category_config()
        
        # ðŸš€ GPU OPTIMIZATION: Force GPU + performance optimizations
        import torch
        torch.backends.cuda.matmul.allow_tf32 = True
        device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'
        
        # ðŸš€ BETTER EMBEDDINGS: BGE-Large for quality
        model_name = 'BAAI/bge-large-en-v1.5'  # Much better than MiniLM
        self.embedder = SentenceTransformer(model_name, device=device)
        if device == 'cuda':
            self.embedder = self.embedder.half()  # Use FP16 for speed
            logger.info(f"ðŸš€ QUALITY EMBEDDINGS: {model_name} on {device} with FP16")
        
        # IMPROVEMENT 4: Enhanced query classification with configurable categories
        self.domain_ontology = self._build_domain_ontology()
        self.category_embeddings = self._build_enhanced_category_embeddings()
        self.technical_terms = self._extract_domain_entities()
        
    def load_config(self) -> Dict:
        """Load domain configuration"""
        if not self.config_file.exists():
            raise FileNotFoundError(f"Domain config not found: {self.config_file}")
            
        with open(self.config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def load_category_config(self) -> Dict:
        """Load adaptive category configuration"""
        if not self.category_config_file.exists():
            logger.warning(f"Category config not found: {self.category_config_file}, using defaults")
            return {}
            
        with open(self.category_config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _build_domain_ontology(self) -> Dict[str, List[str]]:
        """Build domain-specific knowledge ontology for better query understanding"""
        logger.info("Building domain ontology for enhanced query classification...")
        
        ontology = {
            # Audio equipment categories
            "amplifiers": ["tube amp", "solid state", "preamp", "power amp", "integrated amp", "headphone amp"],
            "speakers": ["woofer", "tweeter", "driver", "cabinet", "crossover", "frequency response"],
            "effects": ["reverb", "delay", "chorus", "distortion", "overdrive", "compression"],
            "recording": ["microphone", "preamp", "interface", "daw", "multitrack", "mixing"],
            "technical_specs": ["impedance", "frequency", "power", "thd", "snr", "gain", "decibel"],
            "connections": ["xlr", "trs", "rca", "balanced", "unbalanced", "phantom power"]
        }
        
        # Add terms from domain config if available
        if 'domain_terms' in self.config:
            for category, terms in self.config['domain_terms'].items():
                if category in ontology:
                    ontology[category].extend(terms)
                else:
                    ontology[category] = terms
                    
        return ontology
    
    def _build_enhanced_category_embeddings(self) -> Dict[str, np.ndarray]:
        """Build enhanced embeddings from configurable category definitions"""
        logger.info("Building enhanced category embeddings from adaptive_categories.json...")
        
        # Load categories from JSON config or use defaults
        if 'query_categories' in self.category_config:
            categories = {}
            for cat_name, cat_info in self.category_config['query_categories'].items():
                categories[cat_name] = cat_info['patterns']
        else:
            # Fallback defaults if config not available
            categories = {
                "troubleshooting": ["noise", "problem", "fix", "troubleshoot", "repair", "broken"],
                "setup_operation": ["how to", "setup", "connect", "use", "configure", "operate"],
                "specifications": ["spec", "power", "impedance", "frequency", "rating"],
                "comparison": ["compare", "versus", "difference", "better", "which"],
                "compatibility": ["compatible", "work with", "support", "match"]
            }
        
        embeddings = {}
        for category, examples in categories.items():
            # Add domain-specific terms for each category
            domain_enhanced_examples = examples.copy()
            
            # Add relevant domain terms for better classification
            if category == "technical":
                domain_enhanced_examples.extend(self.domain_ontology.get("technical_specs", []))
            elif category == "comparison":
                # Add equipment comparison terms
                for equipment_type in ["amplifiers", "speakers", "effects"]:
                    domain_enhanced_examples.extend([f"{term} comparison" for term in self.domain_ontology.get(equipment_type, [])])
            
            category_text = " ".join(domain_enhanced_examples)
            embeddings[category] = self.embedder.encode([category_text])[0]
            
        return embeddings
    
    def _extract_domain_entities(self) -> Dict[str, List[str]]:
        """Extract domain-specific entities for better query understanding"""
        logger.info("Extracting domain entities for enhanced query analysis...")
        
        entities = {
            "equipment_types": [],
            "technical_terms": [],
            "brands": [],
            "specifications": []
        }
        
        # Extract from domain ontology
        for category, terms in self.domain_ontology.items():
            if category in ["amplifiers", "speakers", "effects", "recording"]:
                entities["equipment_types"].extend(terms)
            elif category == "technical_specs":
                entities["technical_terms"].extend(terms)
                entities["specifications"].extend(terms)
            elif category == "connections":
                entities["technical_terms"].extend(terms)
        
        # Add common audio brands (basic set)
        entities["brands"] = [
            "fender", "marshall", "vox", "mesa boogie", "orange", "peavey",
            "jbl", "yamaha", "mackie", "shure", "audio-technica", "sennheiser"
        ]
        
        return entities
    
    def analyze_query(self, query: str) -> QueryAnalysis:
        """Enhanced query analysis with domain knowledge and entity recognition"""
        logger.info(f"Analyzing query with enhanced domain understanding: '{query[:100]}...'")
        query_lower = query.lower()
        
        # IMPROVEMENT 4: Enhanced query type classification using domain knowledge
        query_embedding = self.embedder.encode([query])[0]
        
        similarities = {}
        for category, category_embedding in self.category_embeddings.items():
            sim = cosine_similarity([query_embedding], [category_embedding])[0][0]
            similarities[category] = float(sim)
        
        query_type = max(similarities, key=similarities.get)
        logger.info(f"Query type classification: {query_type} (confidence: {similarities[query_type]:.3f})")
        
        # Enhanced domain relevance with ontology-based scoring
        domain_score = self._calculate_enhanced_domain_relevance(query_lower)
        
        # Enhanced complexity assessment with multiple indicators
        complexity = self._assess_query_complexity(query_lower)
        
        # Entity recognition for key concepts
        extracted_entities = self._extract_query_entities(query_lower)
        
        # Advanced uncertainty detection
        uncertainty_required = self._requires_uncertainty_handling(query_lower, domain_score)
        
        # Dynamic confidence threshold based on multiple factors
        confidence_threshold = self._calculate_dynamic_confidence_threshold(
            domain_score, complexity, query_type, len(extracted_entities)
        )
        
        logger.info(f"Query analysis results: type={query_type}, domain={domain_score:.3f}, complexity={complexity}, entities={len(extracted_entities)}, threshold={confidence_threshold:.3f}")
        
        return QueryAnalysis(
            query_type=query_type,
            domain_relevance=domain_score,
            complexity=complexity,
            uncertainty_required=uncertainty_required,
            key_concepts=extracted_entities,
            confidence_threshold=confidence_threshold
        )
    
    def _calculate_enhanced_domain_relevance(self, query_lower: str) -> float:
        """Calculate domain relevance using ontology-based scoring"""
        total_score = 0.0
        max_possible_score = 0.0
        
        # Score based on domain ontology categories
        for category, terms in self.domain_ontology.items():
            category_weight = {
                "amplifiers": 1.0, "speakers": 1.0, "effects": 1.0,
                "recording": 0.9, "technical_specs": 1.2, "connections": 0.8
            }.get(category, 1.0)
            
            matches = sum(1 for term in terms if term.lower() in query_lower)
            category_score = min(matches / max(len(terms) * 0.3, 1), 1.0) * category_weight
            total_score += category_score
            max_possible_score += category_weight
        
        # Normalize score
        domain_relevance = total_score / max_possible_score if max_possible_score > 0 else 0.0
        
        # Boost for exact technical term matches
        technical_boost = sum(0.1 for term in self.technical_terms.get("technical_terms", []) 
                             if term.lower() in query_lower)
        
        return min(domain_relevance + technical_boost, 1.0)
    
    def _assess_query_complexity(self, query_lower: str) -> str:
        """Enhanced complexity assessment with multiple indicators"""
        complexity_scores = {'basic': 0, 'intermediate': 0, 'advanced': 0}
        
        # Pattern-based scoring
        patterns = {
            'basic': [
                r'what is', r'define', r'meaning of', r'purpose of',
                r'simple', r'basic', r'introduction to'
            ],
            'intermediate': [
                r'how does', r'why', r'compare', r'difference between',
                r'relationship', r'interaction', r'effect of', r'works'
            ],
            'advanced': [
                r'calculate', r'optimize', r'design', r'troubleshoot',
                r'specifications?', r'analysis', r'theory', r'mathematical'
            ]
        }
        
        for complexity_level, pattern_list in patterns.items():
            for pattern in pattern_list:
                if re.search(pattern, query_lower):
                    complexity_scores[complexity_level] += 1
        
        # Technical term complexity scoring
        advanced_terms = self.technical_terms.get("specifications", [])
        if any(term.lower() in query_lower for term in advanced_terms):
            complexity_scores['advanced'] += 2
        
        # Query length and structure complexity
        if len(query_lower.split()) > 15:
            complexity_scores['advanced'] += 1
        elif len(query_lower.split()) > 8:
            complexity_scores['intermediate'] += 1
        
        # Multiple entity complexity
        entity_count = sum(len(entities) for entities in self.technical_terms.values())
        if entity_count > 3:
            complexity_scores['advanced'] += 1
        
        return max(complexity_scores, key=complexity_scores.get)
    
    def _extract_query_entities(self, query_lower: str) -> List[str]:
        """Extract domain-specific entities from query"""
        entities = []
        
        # Extract entities from all categories
        for entity_type, entity_list in self.technical_terms.items():
            for entity in entity_list:
                if entity.lower() in query_lower:
                    entities.append(entity)
        
        # Extract from domain ontology
        for category, terms in self.domain_ontology.items():
            for term in terms:
                if term.lower() in query_lower and term not in entities:
                    entities.append(term)
        
        return list(set(entities))  # Remove duplicates
    
    def _requires_uncertainty_handling(self, query_lower: str, domain_score: float) -> bool:
        """Advanced uncertainty detection"""
        # Low domain relevance
        if domain_score < 0.3:
            return True
        
        # Out-of-domain indicators
        out_domain_indicators = [
            'car', 'automobile', 'programming', 'software', 'cooking', 'recipe',
            'weather', 'climate', 'sports', 'game', 'politics', 'news'
        ]
        if any(indicator in query_lower for indicator in out_domain_indicators):
            return True
        
        # Ambiguous query patterns
        ambiguous_patterns = [r'what about', r'tell me about', r'anything about']
        if any(re.search(pattern, query_lower) for pattern in ambiguous_patterns):
            return True
        
        return False
    
    def _calculate_dynamic_confidence_threshold(self, domain_score: float, complexity: str, 
                                               query_type: str, entity_count: int) -> float:
        """Calculate dynamic confidence threshold based on multiple factors"""
        base_threshold = 0.5
        
        # Adjust for domain relevance
        if domain_score > 0.8:
            base_threshold += 0.2
        elif domain_score > 0.6:
            base_threshold += 0.1
        elif domain_score < 0.3:
            base_threshold -= 0.2
        
        # Adjust for complexity
        complexity_adjustments = {
            'basic': 0.1,
            'intermediate': 0.0,
            'advanced': -0.1
        }
        base_threshold += complexity_adjustments.get(complexity, 0.0)
        
        # Adjust for query type confidence
        if query_type in ['specifications', 'troubleshooting']:
            base_threshold += 0.1  # Higher threshold for technical queries
        elif query_type in ['comparison', 'compatibility']:
            base_threshold -= 0.05  # Lower threshold for comparative queries
        
        # Adjust for entity richness
        if entity_count > 3:
            base_threshold += 0.1
        elif entity_count == 0:
            base_threshold -= 0.1
        
        return max(0.2, min(0.9, base_threshold))


class AdaptiveRetriever:
    """ðŸš€ ENHANCED: Adaptive retrieval with hybrid dense+sparse, cross-encoder re-ranking, and dynamic context windows"""
    
    def __init__(self, qa_data: List[Dict], embedder: SentenceTransformer, category_config: Dict = None):
        self.qa_data = qa_data
        self.embedder = embedder
        self.category_config = category_config or {}
        self._track_metrics = True  # Enable stage metrics for debugging
        
        # IMPROVEMENT 1: QUALITY cross-encoder for better re-ranking
        logger.info("ðŸš€ LOADING BGE-Reranker-Large for precision re-ranking...")
        try:
            self.cross_encoder = CrossEncoder('BAAI/bge-reranker-large')  # Large model for quality
            self.has_cross_encoder = True
            logger.info("âœ… BGE-Reranker-Large LOADED - PRECISION RE-RANKING ACTIVE!")
        except Exception as e:
            logger.warning(f"âš ï¸ BGE-Large failed, trying base: {e}")
            try:
                self.cross_encoder = CrossEncoder('BAAI/bge-reranker-base')
                self.has_cross_encoder = True
                logger.info("âœ… BGE-Reranker-Base fallback loaded")
            except Exception as e2:
                logger.error(f"âŒ All rerankers FAILED: {e2}")
                self.cross_encoder = None
                self.has_cross_encoder = False
        
        # Build enhanced indices with hybrid retrieval
        self._build_enhanced_indices()
        
        # IMPROVEMENT 3: Enhanced strategy configurations with dynamic context windows
        self.strategies = {
            'conservative': RetrievalStrategy(
                top_k=3, alpha=0.8, rerank=True, 
                context_window=512, confidence_gating=True, 
                fallback_strategy='uncertainty'
            ),
            'balanced': RetrievalStrategy(
                top_k=5, alpha=0.7, rerank=True,
                context_window=768, confidence_gating=True,
                fallback_strategy='context_expansion'
            ),
            'aggressive': RetrievalStrategy(
                top_k=8, alpha=0.6, rerank=True,
                context_window=1024, confidence_gating=False,
                fallback_strategy='multi_strategy'
            )
        }
    
    def _build_enhanced_indices(self):
        """ðŸš€ IMPROVEMENT 2: Build enhanced hybrid indices: FAISS (dense) + BM25 (sparse) for hybrid retrieval"""
        logger.info("ðŸ—ï¸  Building enhanced hybrid retrieval indices (dense + sparse)...")
        
        # Create multiple text representations for different retrieval strategies
        self.qa_texts = []
        self.question_texts = []
        self.answer_texts = []
        
        for qa in self.qa_data:
            # Combined Q+A for hybrid retrieval
            combined_text = f"Q: {qa['instruction']} A: {qa['output']}"
            self.qa_texts.append(combined_text)
            
            # Separate question and answer texts for targeted retrieval
            self.question_texts.append(qa['instruction'])
            self.answer_texts.append(qa['output'])
        
        # Dense embeddings (multiple strategies)
        logger.info("ðŸ§  Generating dense embeddings for hybrid retrieval...")
        
        # Combined Q+A embeddings (primary)
        combined_embeddings = self.embedder.encode(self.qa_texts, show_progress_bar=True)
        
        # Question-only embeddings (for question similarity)
        question_embeddings = self.embedder.encode(self.question_texts, show_progress_bar=True)
        
        # Answer-only embeddings (for semantic answer matching)
        answer_embeddings = self.embedder.encode(self.answer_texts, show_progress_bar=True)
        
        # Build FAISS indices for different embedding strategies
        logger.info("ðŸ” Building FAISS indices for multiple retrieval strategies...")
        
        # ðŸš€ FIX: Convert FP16 embeddings to FP32 for FAISS compatibility
        if combined_embeddings.dtype != np.float32:
            combined_embeddings = combined_embeddings.astype(np.float32)
        if question_embeddings.dtype != np.float32:
            question_embeddings = question_embeddings.astype(np.float32) 
        if answer_embeddings.dtype != np.float32:
            answer_embeddings = answer_embeddings.astype(np.float32)
        
        # Primary index (combined Q+A)
        dimension = combined_embeddings.shape[1]
        self.combined_index = faiss.IndexFlatIP(dimension)
        faiss.normalize_L2(combined_embeddings)
        self.combined_index.add(combined_embeddings)
        
        # Question similarity index
        self.question_index = faiss.IndexFlatIP(dimension)
        faiss.normalize_L2(question_embeddings)
        self.question_index.add(question_embeddings)
        
        # Answer similarity index
        self.answer_index = faiss.IndexFlatIP(dimension)
        faiss.normalize_L2(answer_embeddings)
        self.answer_index.add(answer_embeddings)
        
        # IMPROVEMENT 2: Proper BM25 + Dense with RRF (0.5/0.5 mix)
        logger.info("ðŸš€ BUILDING BM25 + Dense hybrid with RRF...")
        try:
            from rank_bm25 import BM25Okapi
            
            # Tokenize for BM25
            tokenized_corpus = [text.lower().split() for text in self.qa_texts]
            self.bm25 = BM25Okapi(tokenized_corpus)
            self.has_bm25 = True
            logger.info("âœ… BM25 index BUILT for hybrid retrieval!")
        except Exception as e:
            logger.error(f"âŒ BM25 initialization FAILED: {e}")
            self.bm25 = None
            self.has_bm25 = False
        
        # TF-IDF backup sparse retrieval - FORCE ENABLED
        logger.info("ðŸš€ FORCING TF-IDF backup sparse retrieval...")
        try:
            self.tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.qa_texts)
            self.has_tfidf = True
            logger.info("âœ… TF-IDF backup FORCE BUILT successfully")
        except Exception as e:
            logger.error(f"âŒ TF-IDF FORCE initialization FAILED: {e}")
            self.tfidf_vectorizer = None
            self.tfidf_matrix = None
            self.has_tfidf = False
        
        logger.info(f"ðŸŽ¯ HYBRID RETRIEVAL INDICES COMPLETED:")
        logger.info(f"   ðŸ“Š {len(self.qa_data)} Q&A pairs indexed")
        logger.info(f"   ðŸ§  Dense embeddings: {dimension}D vectors")
        logger.info(f"   ðŸ“ BM25 sparse: {'âœ… ENABLED' if self.has_bm25 else 'âŒ DISABLED - DEGRADED PERFORMANCE!'}")
        logger.info(f"   ðŸ“„ TF-IDF backup: {'âœ… ENABLED' if self.has_tfidf else 'âŒ DISABLED'}")
        logger.info(f"   ðŸ”„ Cross-encoder re-ranking: {'âœ… ENABLED' if self.has_cross_encoder else 'âŒ DISABLED - DEGRADED PERFORMANCE!'}")
        
        # ðŸš€ CRITICAL: Force fix status check with new improvements
        if self.has_sparse and self.has_cross_encoder:
            logger.info("ðŸš€ ADAPTIVE RAG FULLY ENHANCED WITH ALL 3 FIXES - EXPECTING +20-30% PERFORMANCE!")
            logger.info("   âœ… BGE Cross-encoder for technical domain")
            logger.info("   âœ… SPLADE-style sparse retrieval with term expansion")
            logger.info("   âœ… Audio-specific query classification")
        else:
            logger.error("âŒ CRITICAL: ADAPTIVE RAG IS DEGRADED!")
            logger.error(f"   Missing SPLADE sparse: {not self.has_sparse}")
            logger.error(f"   Missing Cross-encoder: {not self.has_cross_encoder}")
            logger.error("âŒ THIS WILL CAUSE PERFORMANCE DEGRADATION!")
            
            # No recovery needed for new SPLADE system - it either works or fails gracefully
    
    def _build_technical_term_expansion(self) -> Dict[str, List[str]]:
        """Build technical term expansion from config or defaults"""
        if 'technical_term_expansion' in self.category_config:
            return self.category_config['technical_term_expansion']
        
        # Fallback defaults
        return {
            "noise": ["artifacts", "interference", "hum", "buzz", "crackle", "static", "distortion"],
            "troubleshoot": ["diagnose", "fix", "repair", "solve", "eliminate", "resolve"],
            "amplifier": ["amp", "preamp", "power amp", "tube amp", "solid state"],
            "speaker": ["driver", "woofer", "tweeter", "cabinet", "monitor"],
            "impedance": ["ohms", "resistance", "load", "matching"],
            "frequency": ["hz", "khz", "response", "range", "bandwidth"],
            "power": ["watts", "wattage", "output", "consumption"],
            "overdrive": ["distortion", "saturation", "clipping", "breakup"]
        }
    
    def _build_technical_vocabulary(self) -> List[str]:
        """Build comprehensive technical vocabulary for sparse retrieval"""
        vocab = set()
        
        # Add base technical terms
        base_terms = [
            "amplifier", "amp", "preamp", "power", "tube", "solid", "state",
            "speaker", "driver", "woofer", "tweeter", "cabinet", "monitor",
            "guitar", "bass", "instrument", "electric", "acoustic", 
            "impedance", "ohms", "frequency", "hz", "khz", "watts", "gain",
            "noise", "hum", "buzz", "interference", "distortion", "artifacts",
            "cable", "wire", "jack", "plug", "input", "output", "connection",
            "overdrive", "reverb", "delay", "chorus", "eq", "treble", "bass",
            "volume", "control", "knob", "switch", "button", "preset"
        ]
        vocab.update(base_terms)
        
        # Add expansion terms
        for term_list in self.term_expansion.values():
            vocab.update(term_list)
            
        # Add technical phrases (2-3 grams)
        technical_phrases = [
            "signal chain", "ground loop", "phantom power", "frequency response",
            "power supply", "input impedance", "output level", "gain stage",
            "tone control", "eq settings", "speaker cabinet", "amp head"
        ]
        vocab.update(technical_phrases)
        
        return list(vocab)

    def select_strategy(self, analysis: QueryAnalysis) -> str:
        """ðŸš€ SAFE ADAPTIVE: Default to Standard, escalate only on low confidence"""
        # Always start with conservative (Standard-like) approach
        return 'conservative'
    
    def should_escalate_to_adaptive(self, query: str, initial_results: List[Dict]) -> bool:
        """ðŸš€ GUARDRAILS: Only escalate when confidence is low"""
        if not initial_results:
            return True  # No results, try adaptive
        
        # Check top-1 dense score
        top_1_score = initial_results[0].get('score', 0.0) if initial_results else 0.0
        
        # Check score gap between top-1 and top-5
        if len(initial_results) >= 5:
            top_5_score = initial_results[4].get('score', 0.0)
            score_gap = top_1_score - top_5_score
        else:
            score_gap = 1.0  # Large gap if we have < 5 results
        
        # Escalation criteria
        low_confidence = top_1_score < 0.30
        small_gap = score_gap < 0.02
        
        escalate = low_confidence and small_gap
        if escalate:
            logger.info(f"ðŸš¨ ESCALATING to adaptive: top_1={top_1_score:.3f}, gap={score_gap:.3f}")
        
        return escalate
    
    def retrieve_adaptive(self, query: str, analysis: QueryAnalysis) -> Tuple[List[Dict], str]:
        """ðŸš€ ENHANCED: Perform adaptive retrieval with hybrid dense+sparse + cross-encoder re-ranking"""
        logger.info(f"ðŸ” ADAPTIVE RETRIEVAL STARTING for: '{query[:50]}...'")
        
        strategy_name = self.select_strategy(analysis)
        strategy = self.strategies[strategy_name]
        
        logger.info(f"ðŸŽ¯ Selected strategy: {strategy_name} (top_k={strategy.top_k}, alpha={strategy.alpha}, rerank={strategy.rerank})")
        
        # ðŸš€ DEBUG: Confirm what retrieval components are active
        logger.info(f"ðŸ”§ RETRIEVAL COMPONENTS STATUS:")
        logger.info(f"   ðŸ“ SPLADE Sparse: {'ACTIVE' if self.has_sparse else 'MISSING - USING DENSE ONLY!'}")
        logger.info(f"   ðŸ”„ Cross-encoder: {'ACTIVE' if self.has_cross_encoder else 'MISSING - NO RERANKING!'}")
        logger.info(f"   ðŸ“Š TF-IDF backup: {'ACTIVE' if self.has_tfidf else 'MISSING'}")
        
        # IMPROVEMENT 2: Hybrid dense + sparse retrieval
        hybrid_candidates = self._hybrid_retrieval(query, analysis, strategy.top_k * 3)  # Get more candidates for re-ranking
        
        # IMPROVEMENT 1: Cross-encoder precision re-ranking (already have top-8 from MMR)
        if strategy.rerank and self.has_cross_encoder and len(hybrid_candidates) > 1:
            logger.info(f"ðŸ”„ Precision re-ranking {len(hybrid_candidates)} candidates with BGE-reranker...")
            reranked_candidates = self._cross_encoder_rerank(query, hybrid_candidates)
        else:
            reranked_candidates = hybrid_candidates
            logger.info("â­ï¸  Skipping re-ranking (disabled or unavailable)")
        
        # IMPROVEMENT 3: Dynamic context window adjustment
        final_results = self._apply_dynamic_context_window(reranked_candidates, strategy, analysis)
        
        # Apply confidence gating and fallback strategies
        final_results = self._apply_confidence_gating(query, analysis, final_results, strategy_name, strategy)
        
        logger.info(f"âœ… Adaptive retrieval completed: {len(final_results)} results returned")
        return final_results, strategy_name
    
    def _hybrid_retrieval(self, query: str, query_analysis: QueryAnalysis, top_k: int = 10) -> List[Tuple[int, float, str]]:
        """ðŸš€ IMPROVEMENT 2: Hybrid dense + sparse retrieval with adaptive weighting"""
        logger.info(f"ðŸ§  PERFORMING HYBRID RETRIEVAL v2.0 (FIXED) for query type: {query_analysis.query_type}")
        logger.info(f"ðŸ”§ This should NOT be 'dense_only' anymore - hybrid retrieval RESTORED!")
        
        # ðŸš€ BREADTH THEN PRECISION: k=50 retrieval â†’ RRF fusion â†’ top-8 diverse
        retrieve_k = 50  # Breadth first
        
        # Get dense results (top-50)
        dense_results = self._get_dense_results(query, query_analysis, retrieve_k)
        
        # Get BM25 results (top-50) 
        bm25_results = self._get_bm25_results(query, retrieve_k) if self.has_bm25 else []
        
        if bm25_results:
            # RRF fusion (0.5/0.5 mix)
            logger.info(f"ðŸ”€ RRF fusion: {len(dense_results)} dense + {len(bm25_results)} BM25")
            fused_candidates = self._rrf_fusion(dense_results, bm25_results)[:retrieve_k]
            retrieval_method = "rrf_hybrid"
        else:
            # Dense-only fallback
            logger.info(f"ðŸ“Š Dense-only: {len(dense_results)} candidates (no BM25)")
            fused_candidates = dense_results
            retrieval_method = "dense_only"
        
        # Apply MMR diversity (Î»=0.5) to get final top-8
        query_embedding = self.embedder.encode([query])[0]
        diverse_candidates = self._mmr_diversify(fused_candidates, query_embedding, lambda_param=0.5, final_k=8)
        
        logger.info(f"ðŸ“Š {retrieval_method.upper()}: {len(fused_candidates)} â†’ {len(diverse_candidates)} diverse candidates")
        return diverse_candidates
    
    def _get_dense_scores(self, query: str, query_analysis: QueryAnalysis, top_k: int) -> Dict[int, float]:
        """ðŸš€ Enhanced dense retrieval with multi-strategy weighting"""
        query_embedding = self.embedder.encode([query])[0].astype('float32')
        query_embedding = query_embedding.reshape(1, -1)
        faiss.normalize_L2(query_embedding)
        
        scores = {}
        
        # Strategy 1: Combined Q+A similarity (primary - weight 0.6)
        combined_scores, combined_indices = self.combined_index.search(query_embedding, min(top_k, len(self.qa_data)))
        for i, (idx, score) in enumerate(zip(combined_indices[0], combined_scores[0])):
            if idx >= 0:  # Valid index
                scores[idx] = float(score) * 0.6
        
        # Strategy 2: Question-only similarity (secondary - weight 0.25)
        q_scores, q_indices = self.question_index.search(query_embedding, min(top_k, len(self.qa_data)))
        for i, (idx, score) in enumerate(zip(q_indices[0], q_scores[0])):
            if idx >= 0:
                scores[idx] = scores.get(idx, 0) + float(score) * 0.25
        
        # Strategy 3: Answer-only similarity (tertiary - weight 0.15)
        a_scores, a_indices = self.answer_index.search(query_embedding, min(top_k, len(self.qa_data)))
        for i, (idx, score) in enumerate(zip(a_indices[0], a_scores[0])):
            if idx >= 0:
                scores[idx] = scores.get(idx, 0) + float(score) * 0.15
        
        return scores
    
    def _get_bm25_scores(self, query: str, top_k: int) -> Dict[int, float]:
        """Get BM25 sparse retrieval scores"""
        scores = {}
        
        if self.has_bm25:
            # BM25 scoring
            query_tokens = query.lower().split()
            bm25_scores = self.bm25.get_scores(query_tokens)
            
            # Get top BM25 matches
            top_bm25_indices = np.argsort(bm25_scores)[::-1][:top_k]
            for idx in top_bm25_indices:
                if idx < len(self.qa_data):
                    scores[idx] = float(bm25_scores[idx])
        
        return scores
    
    def _rrf_fusion(self, dense_results: List[Tuple], bm25_results: List[Tuple], k: int = 60) -> List[Tuple]:
        """ðŸš€ Reciprocal Rank Fusion (RRF) for combining dense + BM25"""
        rrf_scores = {}
        
        # Add dense scores with RRF
        for rank, (idx, score, source) in enumerate(dense_results):
            rrf_scores[idx] = rrf_scores.get(idx, 0) + 1 / (k + rank + 1)
        
        # Add BM25 scores with RRF  
        for rank, (idx, score, source) in enumerate(bm25_results):
            rrf_scores[idx] = rrf_scores.get(idx, 0) + 1 / (k + rank + 1)
        
        # Sort by RRF score
        sorted_items = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)
        
        # Convert back to expected format
        rrf_results = []
        for idx, rrf_score in sorted_items:
            rrf_results.append((idx, rrf_score, "rrf_hybrid"))
        
        return rrf_results
    
    def _mmr_diversify(self, candidates: List[Tuple], query_embedding: np.ndarray, lambda_param: float = 0.5, final_k: int = 8) -> List[Tuple]:
        """ðŸš€ MMR diversity to avoid near-duplicate chunks (Î»=0.5)"""
        if len(candidates) <= final_k:
            return candidates
        
        # Get embeddings for all candidates
        candidate_embeddings = []
        for idx, score, source in candidates:
            if idx < len(self.qa_data):
                # Use answer embeddings for diversity calculation
                answer_text = self.qa_data[idx].get('output', self.qa_data[idx].get('answer', ''))
                emb = self.embedder.encode([answer_text])[0]
                candidate_embeddings.append(emb)
            else:
                candidate_embeddings.append(np.zeros(384))  # Fallback
        
        candidate_embeddings = np.array(candidate_embeddings)
        
        # MMR algorithm
        selected = []
        remaining_indices = list(range(len(candidates)))
        
        # Select first item (highest relevance)
        if remaining_indices:
            best_idx = remaining_indices.pop(0)
            selected.append(candidates[best_idx])
        
        # Select remaining items with MMR
        while len(selected) < final_k and remaining_indices:
            mmr_scores = {}
            
            for i in remaining_indices:
                idx, score, source = candidates[i]
                
                # Relevance score (normalized to 0-1)
                relevance = cosine_similarity([query_embedding], [candidate_embeddings[i]])[0][0]
                
                # Max similarity to already selected items
                max_sim = 0
                if selected:
                    selected_embeddings = [candidate_embeddings[candidates.index(sel_candidate)] for sel_candidate in selected if sel_candidate in candidates]
                    if selected_embeddings:
                        similarities = cosine_similarity([candidate_embeddings[i]], selected_embeddings)[0]
                        max_sim = max(similarities)
                
                # MMR score: Î» * relevance - (1-Î») * max_similarity
                mmr_score = lambda_param * relevance - (1 - lambda_param) * max_sim
                mmr_scores[i] = mmr_score
            
            # Select item with highest MMR score
            if mmr_scores:
                best_idx = max(mmr_scores.keys(), key=lambda x: mmr_scores[x])
                remaining_indices.remove(best_idx)
                selected.append(candidates[best_idx])
        
        logger.info(f"ðŸŽ¯ MMR diversity: {len(candidates)} â†’ {len(selected)} diverse chunks (Î»={lambda_param})")
        return selected
    
    def _get_dense_results(self, query: str, query_analysis: QueryAnalysis, top_k: int) -> List[Tuple]:
        """Get dense retrieval results in (idx, score, source) format"""
        dense_scores = self._get_dense_scores(query, query_analysis, top_k)
        results = [(idx, score, "dense") for idx, score in dense_scores.items()]
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]
    
    def _get_bm25_results(self, query: str, top_k: int) -> List[Tuple]:
        """Get BM25 results in (idx, score, source) format"""
        bm25_scores = self._get_bm25_scores(query, top_k)
        results = [(idx, score, "bm25") for idx, score in bm25_scores.items()]
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]
    
    def _expand_query_terms(self, query: str) -> str:
        """Expand query with technical synonyms for better sparse matching"""
        query_lower = query.lower()
        expanded_terms = [query]  # Always include original
        
        # Add expansion terms for matched concepts
        for base_term, expansions in self.term_expansion.items():
            if base_term in query_lower:
                expanded_terms.extend(expansions[:3])  # Add top 3 expansions
        
        return " ".join(expanded_terms)
    
    def _get_adaptive_alpha(self, query_analysis: QueryAnalysis) -> float:
        """Calculate adaptive weight for dense vs sparse retrieval"""
        base_alpha = 0.7  # Favor dense by default
        
        # Adjust based on audio-specific query type
        if query_analysis.query_type in ['specifications', 'troubleshooting']:
            # Technical queries benefit from exact term matching (sparse)
            base_alpha -= 0.2
        elif query_analysis.query_type in ['comparison', 'compatibility']:
            # Comparative queries benefit from semantic similarity (dense)
            base_alpha += 0.1
        elif query_analysis.query_type == 'setup_operation':
            # Setup queries balance between exact terms and semantic understanding
            base_alpha += 0.0  # Keep default balance
        
        # Adjust based on domain relevance
        if query_analysis.domain_relevance > 0.8:
            # High domain relevance benefits from dense similarity
            base_alpha += 0.1
        elif query_analysis.domain_relevance < 0.3:
            # Low domain relevance benefits from lexical matching
            base_alpha -= 0.2
        
        return max(0.3, min(0.9, base_alpha))
    
    def _combine_hybrid_scores(self, dense_scores: Dict[int, float], sparse_scores: Dict[int, float], alpha: float) -> Dict[int, float]:
        """FIX: Combine dense and sparse scores WITHOUT double normalization"""
        combined_scores = {}
        all_indices = set(dense_scores.keys()) | set(sparse_scores.keys())
        
        # FIX: Dense scores from FAISS are already cosine similarity (0-1 range)
        # FIX: Sparse scores are now pre-normalized in _get_sparse_scores()
        # NO additional normalization needed - this was causing score corruption!
        
        for idx in all_indices:
            dense_score = dense_scores.get(idx, 0.0)  # Already normalized
            sparse_score = sparse_scores.get(idx, 0.0)  # Already normalized
            
            # Simple weighted combination - NO double normalization!
            combined_scores[idx] = alpha * dense_score + (1 - alpha) * sparse_score
        
        return combined_scores
    
    def _cross_encoder_rerank(self, query: str, candidates: List[Tuple[int, float, str]]) -> List[Tuple[int, float, str]]:
        """ðŸš€ IMPROVEMENT 1: Cross-encoder re-ranking for better context scoring - FIXED"""
        if not self.has_cross_encoder or len(candidates) <= 1:
            return candidates
        
        try:
            # Prepare query-document pairs for cross-encoder - FIXED format
            pairs = []
            candidate_indices = []
            
            for idx, score, source in candidates:
                if idx < len(self.qa_data):
                    # FIX: Use answer text directly, not Q+A format for cross-encoder
                    answer_text = self.qa_data[idx]['output']
                    pairs.append([query, answer_text])
                    candidate_indices.append((idx, source))
            
            if not pairs:
                return candidates
            
            # Get cross-encoder scores
            ce_scores = self.cross_encoder.predict(pairs)
            
            # FIX: Use cross-encoder scores directly - they're already meaningful
            ce_scores = np.array(ce_scores)
            
            # Combine with original scores using CORRECT approach
            reranked_candidates = []
            for i, (ce_score, (idx, source)) in enumerate(zip(ce_scores, candidate_indices)):
                original_score = candidates[i][1] 
                
                # FIX: Use cross-encoder score as primary with original as tie-breaker
                # Cross-encoder provides better relevance assessment than initial retrieval
                combined_score = float(ce_score) + 0.1 * original_score  # CE primary, original as boost
                reranked_candidates.append((idx, combined_score, f"reranked_{source}"))
            
            # Sort by combined score
            reranked_candidates.sort(key=lambda x: x[1], reverse=True)
            
            logger.info(f"ðŸ”„ Cross-encoder re-ranking: CE score range [{min(ce_scores):.3f}, {max(ce_scores):.3f}]")
            return reranked_candidates
            
        except Exception as e:
            logger.warning(f"âš ï¸  Cross-encoder re-ranking failed: {e}")
            return candidates
    
    def _apply_dynamic_context_window(self, candidates: List[Tuple[int, float, str]], 
                                    strategy: RetrievalStrategy, analysis: QueryAnalysis) -> List[Dict]:
        """ðŸš€ IMPROVEMENT 3: Apply dynamic context window adjustment"""
        
        # ðŸš€ CONTEXT BUDGETING: 6-8 chunks max, prefer short on-topic
        max_chunks = 8  # Hard limit for context budget
        min_chunks = 6  # Minimum for complex queries
        
        # Filter and prioritize chunks by length and relevance
        chunk_candidates = []
        for idx, score, source in candidates[:12]:  # Consider up to 12 for filtering
            if idx < len(self.qa_data):
                answer_text = self.qa_data[idx].get('output', self.qa_data[idx].get('answer', ''))
                answer_length = len(answer_text.split())
                
                # Prefer shorter, focused answers (100-300 words ideal)
                if 50 <= answer_length <= 300:
                    length_penalty = 1.0  # Ideal length
                elif answer_length < 50:
                    length_penalty = 0.8  # Too short
                elif answer_length <= 500:
                    length_penalty = 0.9  # Acceptable
                else:
                    length_penalty = 0.7  # Too long, rambling
                
                adjusted_score = score * length_penalty
                chunk_candidates.append((idx, adjusted_score, source, answer_length))
        
        # Sort by adjusted score
        chunk_candidates.sort(key=lambda x: x[1], reverse=True)
        
        # Dynamic chunk count based on complexity
        if analysis.complexity == 'advanced':
            target_chunks = min(max_chunks, max(min_chunks, len(chunk_candidates)))
        else:
            target_chunks = min(6, len(chunk_candidates))  # Fewer for simple queries
        
        # Select final chunks
        final_candidates = chunk_candidates[:target_chunks]
        
        # Convert back to expected format (remove length info)
        selected_candidates = [(idx, score, source) for idx, score, source, length in final_candidates]
        
        # Convert to result format
        results = []
        for idx, score, source in selected_candidates:
            if idx < len(self.qa_data):
                result = self.qa_data[idx].copy()
                result['relevance_score'] = float(score)
                result['retrieval_source'] = source
                results.append(result)
        
        # ðŸš€ STAGE METRICS: Track retrieval quality
        if hasattr(self, '_track_metrics') and self._track_metrics:
            self._log_stage_metrics(candidates, results, analysis)
        
        logger.info(f"ðŸ“Š Context budget: {len(candidates)} â†’ {len(results)} chunks (target={target_chunks}, complexity={analysis.complexity})")
        return results
    
    def _log_stage_metrics(self, candidates: List[Tuple], results: List[Dict], analysis: QueryAnalysis):
        """ðŸš€ Log stage metrics for debugging and optimization"""
        if not candidates:
            return
        
        # Retrieval metrics
        scores = [score for _, score, _ in candidates]
        hit_at_1 = 1 if scores and scores[0] > 0.5 else 0
        avg_score = np.mean(scores) if scores else 0
        score_gap = (scores[0] - scores[-1]) if len(scores) > 1 else 0
        
        logger.info(f"ðŸ“ˆ STAGE METRICS:")
        logger.info(f"   Hit@1: {hit_at_1} (top score: {scores[0]:.3f})" if scores else "   Hit@1: 0")
        logger.info(f"   Avg Score: {avg_score:.3f}")
        logger.info(f"   Score Gap: {score_gap:.3f}")
        logger.info(f"   Final Chunks: {len(results)}")
    
    def _apply_confidence_gating(self, query: str, analysis: QueryAnalysis, results: List[Dict], 
                               strategy_name: str, strategy: RetrievalStrategy) -> List[Dict]:
        """Apply confidence gating and fallback strategies"""
        if not strategy.confidence_gating or not results:
            return results
        
        avg_relevance = np.mean([r.get('relevance_score', 0) for r in results])
        
        if avg_relevance < analysis.confidence_threshold:
            logger.info(f"ðŸ”„ Confidence gating triggered: avg_relevance={avg_relevance:.3f} < threshold={analysis.confidence_threshold:.3f}")
            
            # Apply fallback strategy
            if strategy.fallback_strategy == 'uncertainty':
                results = results[:1]  # Use only top result with uncertainty
                logger.info("ðŸ”„ Applied 'uncertainty' fallback: using top result only")
                
            elif strategy.fallback_strategy == 'context_expansion':
                # Expand search with lower threshold (recursive call)
                logger.info("ðŸ”„ Applied 'context_expansion' fallback: expanding search")
                expanded_analysis = replace(analysis, confidence_threshold=0.2)
                return self.retrieve_adaptive(query, expanded_analysis)[0]  # Return only results, not strategy
                
            elif strategy.fallback_strategy == 'multi_strategy':
                # Try different strategy if current one fails
                logger.info("ðŸ”„ Applied 'multi_strategy' fallback: trying different strategy")
                fallback_strategy = 'balanced' if strategy_name == 'conservative' else 'aggressive'
                fallback_analysis = replace(analysis, confidence_threshold=0.3)
                # Note: This could lead to infinite recursion, so we limit fallback depth
                return self.retrieve_adaptive(query, fallback_analysis)[0]
        
        return results


class AdaptiveContextFormatter:
    """Formats context adaptively based on query analysis and retrieval confidence"""
    
    def __init__(self, domain_config: Dict):
        self.config = domain_config
        self.templates = domain_config.get('context_templates', {})
    
    def format_adaptive_context(self, query: str, analysis: QueryAnalysis, 
                              retrieved: List[Dict], strategy_used: str) -> str:
        """Format context adaptively based on analysis and retrieval quality"""
        
        # Calculate context quality
        avg_relevance = np.mean([r.get('relevance_score', 0) for r in retrieved]) if retrieved else 0
        
        # Select template based on query type
        if analysis.query_type == 'comparison':
            template = self.templates.get('comparison', {})
        elif analysis.query_type in ['technical', 'troubleshooting']:
            template = self.templates.get('technical', {})
        else:
            template = self.templates.get('general', {})
        
        # Determine confidence level for instructions
        if avg_relevance >= 0.7:
            confidence_level = 'high'
        elif avg_relevance >= 0.4:
            confidence_level = 'medium'
        else:
            confidence_level = 'low'
        
        # Build context
        prefix = template.get('prefix', f"{self.config.get('domain_info', {}).get('domain', 'DOMAIN').upper()} REFERENCE INFORMATION\n\n")
        suffix = template.get('suffix', "Question: {question}\n\n")
        
        context = prefix
        
        # Add retrieved information with confidence indicators
        for i, item in enumerate(retrieved, 1):
            relevance = item.get('relevance_score', 0)
            confidence_indicator = "ðŸ”´" if relevance < 0.3 else "ðŸŸ¡" if relevance < 0.6 else "ðŸŸ¢"
            
            context += f"Reference {i} {confidence_indicator}:\n"
            context += f"  Q: {item['instruction']}\n"
            context += f"  A: {item['output']}\n"
            context += f"  Relevance: {relevance:.2f}\n\n"
        
        context += suffix.format(question=query)
        
        # Add adaptive instructions
        confidence_instructions = template.get('confidence_instructions', {})
        if confidence_level in confidence_instructions:
            context += confidence_instructions[confidence_level] + "\n"
        
        # Add strategy-specific guidance
        if strategy_used == 'conservative':
            context += "\nNOTE: Conservative retrieval used - express uncertainty if information is insufficient.\n"
        elif strategy_used == 'aggressive':
            context += f"\nNOTE: Comprehensive search performed ({len(retrieved)} references) - provide detailed analysis.\n"
        
        # Add domain boundary guidance
        if analysis.domain_relevance < 0.5:
            context += "\nIMPORTANT: This question may be outside the domain scope. Express appropriate uncertainty.\n"
        
        return context


class AdaptiveRAGPipeline:
    """Complete adaptive RAG pipeline"""
    
    def __init__(self, qa_data: List[Dict], domain_config_file: str = "audio_equipment_domain_questions.json"):
        self.qa_data = qa_data
        
        # ðŸš€ DEBUG: Confirm pipeline version and fixes
        logger.info("ðŸš€ ADAPTIVE RAG PIPELINE v2.0 - ENHANCED VERSION LOADING...")
        logger.info("âœ… All 4 improvements should be ENABLED:")
        logger.info("   1. Cross-encoder re-ranking")
        logger.info("   2. Hybrid dense+sparse retrieval") 
        logger.info("   3. Dynamic context windows")
        logger.info("   4. Enhanced query classification")
        
        # Initialize components
        self.analyzer = QueryAnalyzer(domain_config_file)
        self.retriever = AdaptiveRetriever(qa_data, self.analyzer.embedder, self.analyzer.category_config)
        self.formatter = AdaptiveContextFormatter(self.analyzer.config)
        
        logger.info(f"Adaptive RAG Pipeline initialized with {len(qa_data)} Q&A pairs")
    
    def process_query(self, query: str) -> Dict:
        """Process query through adaptive RAG pipeline"""
        
        # 1. Query Analysis
        analysis = self.analyzer.analyze_query(query)
        logger.info(f"Query analysis: {analysis.query_type} | Domain: {analysis.domain_relevance:.2f} | {analysis.complexity}")
        
        # 2. Adaptive Retrieval
        retrieved, strategy_used = self.retriever.retrieve_adaptive(query, analysis)
        logger.info(f"Retrieved {len(retrieved)} items using {strategy_used} strategy")
        
        # 3. Context Formatting
        formatted_context = self.formatter.format_adaptive_context(query, analysis, retrieved, strategy_used)
        
        # 4. Calculate context quality metrics
        avg_relevance = np.mean([r.get('relevance_score', 0) for r in retrieved]) if retrieved else 0
        context_diversity = len(set(r.get('difficulty', 'unknown') for r in retrieved)) if retrieved else 0
        
        # 5. Prepare adaptive metadata
        adaptive_adjustments = []
        if strategy_used == 'conservative':
            adaptive_adjustments.append("Used conservative retrieval due to low domain relevance")
        if analysis.uncertainty_required:
            adaptive_adjustments.append("Uncertainty expression recommended")
        if avg_relevance < analysis.confidence_threshold:
            adaptive_adjustments.append("Low confidence context - increased uncertainty warranted")
        
        return {
            'query': query,
            'analysis': analysis,
            'retrieved_items': retrieved,
            'strategy_used': strategy_used,
            'formatted_context': formatted_context,
            'context_quality': avg_relevance,
            'context_diversity': context_diversity,
            'adaptive_adjustments': adaptive_adjustments,
            'confidence_threshold': analysis.confidence_threshold
        }
    
    def evaluate_adaptation(self, queries: List[str]) -> pd.DataFrame:
        """Evaluate adaptive pipeline on multiple queries"""
        results = []
        
        for query in queries:
            try:
                result = self.process_query(query)
                
                results.append({
                    'query': query,
                    'query_type': result['analysis'].query_type,
                    'domain_relevance': result['analysis'].domain_relevance,
                    'complexity': result['analysis'].complexity,
                    'strategy_used': result['strategy_used'],
                    'num_retrieved': len(result['retrieved_items']),
                    'context_quality': result['context_quality'],
                    'context_diversity': result['context_diversity'],
                    'num_adjustments': len(result['adaptive_adjustments']),
                    'uncertainty_required': result['analysis'].uncertainty_required
                })
                
            except Exception as e:
                logger.error(f"Error processing query '{query}': {e}")
                results.append({
                    'query': query,
                    'error': str(e)
                })
        
        return pd.DataFrame(results)


def main():
    """Test the adaptive RAG pipeline"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Adaptive RAG Pipeline')
    parser.add_argument('--qa-data', default='rag_input/selected_qa_pairs.json', 
                       help='Q&A pairs JSON file')
    parser.add_argument('--config', default='audio_equipment_domain_questions.json',
                       help='Domain configuration file')
    parser.add_argument('--test-queries', nargs='+', 
                       default=['What is impedance matching?', 
                               'How do you change a tire?',
                               'Compare tube vs solid-state amplifiers'],
                       help='Test queries')
    parser.add_argument('--output-dir', default='adaptive_rag_results',
                       help='Output directory')
    
    args = parser.parse_args()
    
    # Load Q&A data
    logger.info(f"Loading Q&A data from {args.qa_data}")
    with open(args.qa_data, 'r') as f:
        qa_data = json.load(f)
    
    # Initialize pipeline
    pipeline = AdaptiveRAGPipeline(qa_data, args.config)
    
    # Process test queries
    logger.info("Processing test queries...")
    for query in args.test_queries:
        logger.info(f"\n{'='*60}")
        logger.info(f"QUERY: {query}")
        logger.info('='*60)
        
        result = pipeline.process_query(query)
        
        print(f"\nQuery Type: {result['analysis'].query_type}")
        print(f"Domain Relevance: {result['analysis'].domain_relevance:.3f}")
        print(f"Complexity: {result['analysis'].complexity}")
        print(f"Strategy Used: {result['strategy_used']}")
        print(f"Items Retrieved: {len(result['retrieved_items'])}")
        print(f"Context Quality: {result['context_quality']:.3f}")
        print(f"Adaptive Adjustments: {result['adaptive_adjustments']}")
        
        print(f"\nFormatted Context Preview:")
        print(result['formatted_context'][:500] + "..." if len(result['formatted_context']) > 500 else result['formatted_context'])
    
    # Evaluation
    logger.info("\nRunning evaluation...")
    eval_df = pipeline.evaluate_adaptation(args.test_queries)
    
    # Save results
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    eval_file = output_dir / "adaptive_rag_evaluation.csv"
    eval_df.to_csv(eval_file, index=False)
    logger.info(f"Evaluation results saved to {eval_file}")
    
    # Summary
    print("\n" + "="*60)
    print("ADAPTIVE RAG PIPELINE SUMMARY")
    print("="*60)
    print(f"Queries processed: {len(args.test_queries)}")
    print(f"Average context quality: {eval_df['context_quality'].mean():.3f}")
    print(f"Strategy distribution:")
    for strategy, count in eval_df['strategy_used'].value_counts().items():
        print(f"  {strategy}: {count}")
    print(f"Query type distribution:")
    for qtype, count in eval_df['query_type'].value_counts().items():
        print(f"  {qtype}: {count}")


if __name__ == "__main__":
    main()